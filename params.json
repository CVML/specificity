{"name":"specificity","tagline":"","body":"### Abstract\r\nFor some images, descriptions written by multiple people are consistent with each other. But for other images, descriptions across people vary considerably. In other words, some images are specific − they elicit consistent descriptions from different people − while other images are ambiguous. Applications involving images and text can benefit from an understanding of which images are specific and which ones are ambiguous. For instance, consider text-based image retrieval. If a query description is moderately similar to the caption (or reference description) of an ambiguous image, that query may be considered a decent match to the image. But if the image is very specific, a moderate similarity between the query and the reference description may not be sufficient to retrieve the image.\r\n\r\nIn this paper, we introduce the notion of image specificity. We present two mechanisms to measure specificity given multiple descriptions of an image: an automated measure and a measure that relies on human judgement. We analyze image specificity with respect to image content and properties to better understand what makes an image specific. We then train models to automatically predict the specificity of an image from image features alone without requiring textual descriptions of the image. Finally, we show that modeling image specificity leads to improvements in a text-based image retrieval application. \r\n\r\n### BibTeX\r\n@inproceedings{jas2015specificity,\r\nAuthor = {Mainak Jas and Devi Parikh},\r\nTitle = {{Image Specificity}},\r\nYear = {2015},\r\nbooktitle = {{IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}}\r\n}\r\n\r\n### Authors\r\nMainak Jas\r\nDevi Parikh\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}